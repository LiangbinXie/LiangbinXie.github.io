<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>VFHQ: A High-Quality Dataset and Benchmark for Video Face Super-Resolution</title>
  <!--=================Meta tags==========================-->
  <meta name="robots" content="index,follow">
  <meta name="description"
    content="Most of the existing video face super-resolution (VFSR) methods are trained and evaluated on VoxCeleb1, which is designed specifically for speaker identification and the frames in this dataset are of low quality. As a consequence, the VFSR models trained on this dataset can not output visual-pleasing results. In this paper, we develop an automatic and scalable pipeline to collect a high-quality video face dataset (VFHQ), which contains over $16,000$ high-fidelity clips of diverse interview scenarios. To verify the necessity of VFHQ, we further conduct experiments and demonstrate that VFSR models trained on our VFHQ dataset can generate results with sharper edges and finer textures than those trained on VoxCeleb1. In addition, we show that the temporal information plays a pivotal role in eliminating video consistency issues as well as further improving visual performance. Based on VFHQ, by analyzing the benchmarking study of several state-of-the-art algorithms under bicubic and blind settings.">
  <meta name="keywords" content="Video Face super resolution, VFHQ">
  <link rel="author" href="https://LiangbinXie.github.io/projects/vfhq">
  <!--=================js==========================-->
  <link href="./css.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" type="text/css" href="./project.css" media="screen">
  <script src="./effect.js "></script>
  <!-- Latex -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
      </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
    </script>
  <!--=================Google Analytics==========================-->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-129775907-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-129775907-1');
  </script>
</head>

<body>
  <div id="content">
    <div id="content-inner">
      <div class="section head">
        <h1>
          <font color="Tomato">VFHQ</font>: A <font color="Tomato">H</font>igh-<font color="Tomato">Q</font>uality
          Dataset and Benchmark
        </h1>
        <h1>
          for <font color="Tomato">V</font>ideo <font color="Tomato">F</font>ace Super Resolution
        </h1>
        <!--=================Authors==========================-->
        <div class="authors">
          <a href="https://LiangbinXie.github.io/" target="_blank">Liangbin Xie</a>
          <sup>1,2,3</sup> &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://xinntao.github.io/" target="_blank">Xintao Wang</a>
          <sup>3</sup> &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://scholar.google.com/citations?hl=en&user=KjQLROoAAAAJ" target="_blank">Honglun Zhang</a>
          <sup>3</sup>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://scholar.google.com.hk/citations?user=OSDCB0UAAAAJ&hl=zh-CN" target="_blank">Chao Dong</a>
          <sup>1*</sup>
          &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://scholar.google.com/citations?user=4oXBp9UAAAAJ&hl=en" target="_blank">Ying Shan</a>
          <sup>3</sup>
        </div>

        <div class="affiliations ">
          <sup>1</sup> Shenzhen Key Lab of Computer Vision and Pattern Recognition, <br>
          Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences <br>
          <sup>2</sup> University of Chinese Academy of Sciences<br>
          <sup>3</sup> ARC Lab, Tencent PCG
        </div>

        <!--=================Tabs==========================-->
        <ul id="tabs">
          <li><a href="#license" name="#tab1">License</a></li>
          <li><a href="#materials" name="#tab2">Materials</a></li>
          <!-- <li><a href="#results" name="#tab4">Results</a></li> -->
          <li><a href="#citation" name="#tab5">Citation</a></li>
      </div>
      <br>
      <!--=================Teasers==========================-->


      <!-- <div id="img_intro_examples" class="img_container">
        <center>
          <div class="leftView">
            <div class="mask" style="width:80px;height:80px"></div>
            <img class='small' src="./GFPGAN_src/gfpgan_teaser.jpg">
          </div>
        </center>
      </div>
      <div class="section">
        <p>Comparisons with state-of-the-art face restoration methods: HiFaceGAN, DFDNet, Wan et al.
          and
          PULSE on the real-world low-quality images. While previous methods struggle to restore faithful facial
          details or
          retain
          face identity, our proposed GFP-GAN achieves a good balance of realness and fidelity with much less artifacts.
          In
          addition,
          the powerful generative facial prior allows us to perform restoration and color enhancement jointly.
        </p>
      </div> -->

      <div class="img_container" , id="img_intro_examples">
        <table width="100%" align="center" border=none cellspacing="0" cellpadding="30">
          <tr>
            <td width="50%">
              <center>
                <img src="./VFHQ_src/vfhq_teasor.png" , width="100%"></a><br><br>
                <small>
                  Visual comparison between BasicVSR-GAN models trained with Voxceleb1 and <br>
                  VFHQ dataset, respectively. The high-quality VFHQ dataset helps to recover <br>
                  more visual-pleasing results with finer details.</small>
              </center>
            </td>
            <td width="40%" valign="middle">
              <center>
                <img src="./VFHQ_src/comparison.png" , width="85%"></a><br><br><br>
                <small>
                  Visual comparisons between the two datasets: <br>
                  VoxCeleb1 (<strong>top</strong>) and VFHQ (<strong>bottom</strong>). Images <br>
                  are randomly selected from the dataset. <br>
                  VFHQ images have much higher quality.</small>
              </center>
            </td>
          </tr>
        </table>
      </div>

      <!--=================Abstract==========================-->
      <div class="section abstract">
        <h2>Abstract</h2>
        <br>
        <p>
          Most of the existing video face super-resolution (VFSR) methods are trained and evaluated on VoxCeleb1, which
          is
          designed specifically for speaker identification and the frames in this dataset are of low quality. As a
          consequence,
          the VFSR models trained on this dataset can not output visual-pleasing results. In this paper, we develop an
          automatic
          and scalable pipeline to collect a high-quality video face dataset (VFHQ), which contains over $16,000$
          high-fidelity
          clips of diverse interview scenarios. To verify the necessity of VFHQ, we further conduct experiments and
          demonstrate
          that VFSR models trained on our VFHQ dataset can generate results with sharper edges and finer textures than
          those
          trained on VoxCeleb1. In addition, we show that the temporal information plays a pivotal role in eliminating
          video
          consistency issues as well as further improving visual performance. Based on VFHQ, by analyzing the
          benchmarking study
          of several state-of-the-art algorithms under bicubic and blind settings.
        </p>
      </div>

      <!--=================License==========================-->
      <div class="section license" , id="license">
        <h2>License</h2>
        The VFHQ dataset is only available to download for research purpose under <a
          href="https://creativecommons.org/licenses/by/4.0/">a Creative
          Commons
          Attribution 4.0
          International License</a>. The copyright remains with the original owners of the video. A complete version of
        the
        license
        can be found <a href="VFHQ_src/files/license.txt">here</a> and we refer to the license of <a
          href="https://www.robots.ox.ac.uk/~vgg/data/voxceleb/">VoxCeleb</a>. <br><br>
        <strong>Caution</strong>: We note that the distribution of identities in the VFHQ datasets may not be
        representative of the global human
        population. Please be careful of unintended societal, gender, racial and other biases when training or deploying
        models
        trained on this data.
      </div>

      <!--=================Materials==========================-->
      <div class="section materials" , id="materials">
        <h2>Materials</h2>
        <table width="100%" align="center" border=none cellspacing="0" cellpadding="30">
          <tr>
            <td width="60%">
              <center>
                <a href="https://arxiv.org/abs/2101.04061" target="_blank" class="imageLink"><img
                    src="./VFHQ_src/paper_thumbnail.jpg" , width="80%"></a><br><br>
                <a href="https://arxiv.org/abs/2101.04061" target="_blank">Paper</a>
              </center>
            </td>
            <td width="50%" valign="middle">
              <br>
              <center>
                <img src="./folders.png" , width="30%"><br><br>
                <span class="block-text">
                  <a href="https://1drv.ms/u/s!Ag1HH_EDGMqqh2OkSCrp4Ql3i17Q?e=ZTvUAP" target="_blank">
                    <strong>&nbsp;VFHQ (OneDrive)&nbsp;</strong></a></span><br><br>
                <span class=" block-text">
                  <a href="https://share.weiyun.com/RB8c5NlN" target="_blank"> <strong>&nbsp;VFHQ
                      (腾讯微云)&nbsp;</strong></a>
                  <p align="center">The script about how to extract high-resolution faces with meta info is also
                    included.
                  </p>
                </span>
              </center>
            </td>
          </tr>
        </table>
      </div>
      <!--=================Dataset Description==========================-->
      <div class="section Description" , id="description">
        <h2>Dataset Description</h2>
        <table width="100%" align="center" border=none cellspacing="0" cellpadding="30">
          <center>
            <img src="./VFHQ_src/description.jpg" , width="100%"></a><br><br>
            <p align="left"><small>Distribution of the properties of the celebrities in our
                VFHQ in different aspects. As shown in (a), VFHQ includes persons
                that come from more than 20 distinct countries. In (b), we
                notice that the proportion of men and women is roughly the same.The figure (c) demonstrates that the
                distribution of clip resolution
                of our VFHQ is different from VoxCeleb1 and the resolution of
                VFHQ is much higher than VoxCeleb1. Above the bar is the number
                of clips. Note that we use the length of the shortest side as the
                clip resolution. The figure (d) shows that the quality of VFHQ is
                higher than VoxCeleb1 quantitatively.</small></p>
          </center>
        </table>
      </div>



      <!--=================Citation==========================-->
      <div class="section citation" , id="citation">
        <h2>Citation</h2>
        <div class="section bibtex">
          <pre>@InProceedings{xie2022vfhq,
          author = {Liangbin Xie and Xintao Wang and Honglun Zhang and Chao Dong and Ying Shan},
          title = {VFHQ: A High-Quality Dataset and Benchmark for Video Face Super-Resolution},
          booktitle={The IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
          year = {2022}
          }
          </pre>
        </div>
      </div>
      <!--=================Contact==========================-->
      <div class="section contact">
        <h2 id="contact">Contact</h2>
        <p>If you have any question, please contact Liangbin Xie at <strong>lb.xie@siat.ac.cn</strong>.</p>
      </div>
</body>

</html>